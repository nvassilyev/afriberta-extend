{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/miniconda3/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# afriberta = AutoModelForMaskedLM.from_pretrained(\"castorini/afriberta_large\")\n",
    "tokenizer_afriberta = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "\n",
    "\n",
    "# xlmr = AutoModelForMaskedLM.from_pretrained(\"nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large\")\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(\"nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large\")\n",
    "\n",
    "# xlmr.save_pretrained(\"../models/xlmr_distilled\")\n",
    "# afriberta.save_pretrained(\"../models/afriberta\")\n",
    "\n",
    "afriberta = AutoModelForMaskedLM.from_pretrained(\"../models/afriberta\")\n",
    "xlmr = AutoModelForMaskedLM.from_pretrained(\"../models/xlmr_distilled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'مرحبا', '▁', 'كيف', '▁', 'حا', 'ل', 'ك']\n",
      "['▁م', 'رحب', 'ا', '▁كيف', '▁حال', 'ك']\n"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا كيف حالك\"\n",
    "\n",
    "print(tokenizer_afriberta.tokenize(text))\n",
    "print(tokenizer_xlmr.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70006, 768])\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.0066, -0.0544, -0.0122,  ...,  0.0253, -0.0409,  0.0022],\n",
      "        [ 0.0014, -0.0939, -0.0466,  ...,  0.0630, -0.1047, -0.0675],\n",
      "        [ 0.0011, -0.0057,  0.0189,  ..., -0.0010,  0.0760,  0.0195],\n",
      "        ...,\n",
      "        [ 0.0105, -0.0434, -0.0208,  ...,  0.0209, -0.0526,  0.0214],\n",
      "        [-0.0286,  0.0250, -0.0174,  ..., -0.0474,  0.0257, -0.0088],\n",
      "        [ 0.0075, -0.0547, -0.0067,  ..., -0.0089,  0.0010, -0.0029]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "embedding = model.base_model.get_input_embeddings()\n",
    "copy = embedding.weight.clone()\n",
    "\n",
    "print(embedding.weight.shape)\n",
    "print(type(embedding))\n",
    "print(type(embedding.weight))\n",
    "print(type(embedding.weight.data))\n",
    "print(embedding.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0192,  0.0456,  0.0017,  ..., -0.0313,  0.0531, -0.0389],\n",
       "        [-0.0191,  0.0401, -0.0203,  ..., -0.0047,  0.1012, -0.0017],\n",
       "        [-0.0140,  0.0728, -0.0112,  ..., -0.0900,  0.0288,  0.0364],\n",
       "        ...,\n",
       "        [ 0.0071, -0.0141, -0.0514,  ..., -0.0081, -0.0669, -0.0096],\n",
       "        [ 0.0232, -0.0285, -0.0173,  ..., -0.0430,  0.0374, -0.0250],\n",
       "        [-0.0344, -0.0044, -0.0261,  ..., -0.1030,  0.0151,  0.0507]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = embedding.weight.shape[0]\n",
    "permutation = torch.randperm(vocab_size)\n",
    "\n",
    "embedding.weight.data = embedding.weight.data[permutation, :]\n",
    "model.base_model.set_input_embeddings(embedding)\n",
    "model.base_model.get_input_embeddings().weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/afriberta_rand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current max_split_size_mb: N/A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_value = os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\", \"N/A\")\n",
    "print(\"Current max_split_size_mb:\", current_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0192,  0.0456,  0.0017,  ..., -0.0313,  0.0531, -0.0389],\n",
       "        [-0.0191,  0.0401, -0.0203,  ..., -0.0047,  0.1012, -0.0017],\n",
       "        [-0.0140,  0.0728, -0.0112,  ..., -0.0900,  0.0288,  0.0364],\n",
       "        ...,\n",
       "        [ 0.0071, -0.0141, -0.0514,  ..., -0.0081, -0.0669, -0.0096],\n",
       "        [ 0.0232, -0.0285, -0.0173,  ..., -0.0430,  0.0374, -0.0250],\n",
       "        [-0.0344, -0.0044, -0.0261,  ..., -0.1030,  0.0151,  0.0507]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afriberta_rand = AutoModelForMaskedLM.from_pretrained(\"../models/afriberta_rand\")\n",
    "afriberta_rand.base_model.get_input_embeddings().weight.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
