{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/miniconda3/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# afriberta = AutoModelForMaskedLM.from_pretrained(\"castorini/afriberta_large\")\n",
    "tokenizer_afriberta = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "\n",
    "\n",
    "# xlmr = AutoModelForMaskedLM.from_pretrained(\"nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large\")\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(\"nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large\")\n",
    "\n",
    "# xlmr.save_pretrained(\"../models/xlmr_distilled\")\n",
    "# afriberta.save_pretrained(\"../models/afriberta\")\n",
    "\n",
    "afriberta = AutoModelForMaskedLM.from_pretrained(\"../models/afriberta\")\n",
    "xlmr = AutoModelForMaskedLM.from_pretrained(\"../models/xlmr_distilled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'مرحبا', '▁', 'كيف', '▁', 'حا', 'ل', 'ك']\n",
      "['▁م', 'رحب', 'ا', '▁كيف', '▁حال', 'ك']\n"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا كيف حالك\"\n",
    "\n",
    "print(tokenizer_afriberta.tokenize(text))\n",
    "print(tokenizer_xlmr.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XLMRobertaForMaskedLM(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 384, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 384, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 384)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): XLMRobertaLMHead(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=384, out_features=250002, bias=True)\n",
      "  )\n",
      ")\n",
      "roberta XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 384, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 384, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 384)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.embeddings XLMRobertaEmbeddings(\n",
      "  (word_embeddings): Embedding(250002, 384, padding_idx=1)\n",
      "  (position_embeddings): Embedding(514, 384, padding_idx=1)\n",
      "  (token_type_embeddings): Embedding(1, 384)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.embeddings.word_embeddings Embedding(250002, 384, padding_idx=1)\n",
      "roberta.embeddings.position_embeddings Embedding(514, 384, padding_idx=1)\n",
      "roberta.embeddings.token_type_embeddings Embedding(1, 384)\n",
      "roberta.embeddings.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.embeddings.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder XLMRobertaEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x XLMRobertaLayer(\n",
      "      (attention): XLMRobertaAttention(\n",
      "        (self): XLMRobertaSelfAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): XLMRobertaSelfOutput(\n",
      "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): XLMRobertaIntermediate(\n",
      "        (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): XLMRobertaOutput(\n",
      "        (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer ModuleList(\n",
      "  (0-11): 12 x XLMRobertaLayer(\n",
      "    (attention): XLMRobertaAttention(\n",
      "      (self): XLMRobertaSelfAttention(\n",
      "        (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): XLMRobertaSelfOutput(\n",
      "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): XLMRobertaIntermediate(\n",
      "      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): XLMRobertaOutput(\n",
      "      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.0.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.0.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.0.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.0.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.0.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.0.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.0.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.0.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.1.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.1.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.1.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.1.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.1.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.1.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.1.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.1.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.2.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.2.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.2.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.2.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.2.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.2.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.2.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.2.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.3.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.3.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.3.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.3.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.3.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.3.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.3.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.3.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.4.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.4.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.4.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.4.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.4.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.4.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.4.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.4.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.5.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.5.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.5.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.5.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.5.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.5.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.5.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.5.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.6.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.6.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.6.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.6.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.6.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.6.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.6.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.6.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.7.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.7.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.7.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.7.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.7.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.7.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.7.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.7.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.8.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.8.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.8.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.8.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.8.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.8.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.8.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.8.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.9.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.9.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.9.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.9.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.9.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.9.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.9.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.9.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.10 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.10.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.10.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.10.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.10.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.10.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.10.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.10.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.10.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.10.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.10.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.10.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.10.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.10.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.10.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.10.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.10.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.11 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.11.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.11.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.11.attention.self.query Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.11.attention.self.key Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.11.attention.self.value Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.11.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.11.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.11.attention.output.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.11.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.11.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.11.intermediate.dense Linear(in_features=384, out_features=1536, bias=True)\n",
      "roberta.encoder.layer.11.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.11.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (LayerNorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.11.output.dense Linear(in_features=1536, out_features=384, bias=True)\n",
      "roberta.encoder.layer.11.output.LayerNorm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.11.output.dropout Dropout(p=0.1, inplace=False)\n",
      "lm_head XLMRobertaLMHead(\n",
      "  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder): Linear(in_features=384, out_features=250002, bias=True)\n",
      ")\n",
      "lm_head.dense Linear(in_features=384, out_features=384, bias=True)\n",
      "lm_head.layer_norm LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head.decoder Linear(in_features=384, out_features=250002, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = xlmr\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XLMRobertaForMaskedLM(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(70006, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-9): 10 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): XLMRobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=70006, bias=True)\n",
      "  )\n",
      ")\n",
      "roberta XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(70006, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-9): 10 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.embeddings XLMRobertaEmbeddings(\n",
      "  (word_embeddings): Embedding(70006, 768, padding_idx=1)\n",
      "  (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "  (token_type_embeddings): Embedding(1, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.embeddings.word_embeddings Embedding(70006, 768, padding_idx=1)\n",
      "roberta.embeddings.position_embeddings Embedding(514, 768, padding_idx=1)\n",
      "roberta.embeddings.token_type_embeddings Embedding(1, 768)\n",
      "roberta.embeddings.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.embeddings.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder XLMRobertaEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-9): 10 x XLMRobertaLayer(\n",
      "      (attention): XLMRobertaAttention(\n",
      "        (self): XLMRobertaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): XLMRobertaSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): XLMRobertaIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): XLMRobertaOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer ModuleList(\n",
      "  (0-9): 10 x XLMRobertaLayer(\n",
      "    (attention): XLMRobertaAttention(\n",
      "      (self): XLMRobertaSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): XLMRobertaSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): XLMRobertaIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): XLMRobertaOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.0.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.0.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.0.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.0.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.0.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.0.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.0.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.0.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.0.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.0.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.0.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.1.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.1.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.1.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.1.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.1.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.1.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.1.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.1.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.1.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.1.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.1.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.2.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.2.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.2.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.2.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.2.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.2.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.2.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.2.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.2.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.2.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.2.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.3.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.3.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.3.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.3.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.3.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.3.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.3.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.3.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.3.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.3.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.3.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.4.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.4.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.4.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.4.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.4.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.4.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.4.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.4.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.4.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.4.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.4.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.5.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.5.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.5.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.5.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.5.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.5.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.5.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.5.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.5.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.5.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.5.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.6.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.6.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.6.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.6.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.6.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.6.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.6.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.6.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.6.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.6.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.6.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.7.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.7.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.7.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.7.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.7.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.7.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.7.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.7.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.7.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.7.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.7.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.8.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.8.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.8.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.8.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.8.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.8.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.8.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.8.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.8.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.8.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.8.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9 XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.9.attention XLMRobertaAttention(\n",
      "  (self): XLMRobertaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): XLMRobertaSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "roberta.encoder.layer.9.attention.self XLMRobertaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.9.attention.self.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9.attention.output XLMRobertaSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.9.attention.output.dropout Dropout(p=0.1, inplace=False)\n",
      "roberta.encoder.layer.9.intermediate XLMRobertaIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "roberta.encoder.layer.9.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
      "roberta.encoder.layer.9.intermediate.intermediate_act_fn GELUActivation()\n",
      "roberta.encoder.layer.9.output XLMRobertaOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "roberta.encoder.layer.9.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
      "roberta.encoder.layer.9.output.LayerNorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "roberta.encoder.layer.9.output.dropout Dropout(p=0.1, inplace=False)\n",
      "lm_head XLMRobertaLMHead(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder): Linear(in_features=768, out_features=70006, bias=True)\n",
      ")\n",
      "lm_head.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "lm_head.layer_norm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head.decoder Linear(in_features=768, out_features=70006, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = afriberta\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70006, 768])\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.0066, -0.0544, -0.0122,  ...,  0.0253, -0.0409,  0.0022],\n",
      "        [ 0.0014, -0.0939, -0.0466,  ...,  0.0630, -0.1047, -0.0675],\n",
      "        [ 0.0011, -0.0057,  0.0189,  ..., -0.0010,  0.0760,  0.0195],\n",
      "        ...,\n",
      "        [ 0.0105, -0.0434, -0.0208,  ...,  0.0209, -0.0526,  0.0214],\n",
      "        [-0.0286,  0.0250, -0.0174,  ..., -0.0474,  0.0257, -0.0088],\n",
      "        [ 0.0075, -0.0547, -0.0067,  ..., -0.0089,  0.0010, -0.0029]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "embedding = model.base_model.get_input_embeddings()\n",
    "copy = embedding.weight.clone()\n",
    "\n",
    "print(embedding.weight.shape)\n",
    "print(type(embedding))\n",
    "print(type(embedding.weight))\n",
    "print(type(embedding.weight.data))\n",
    "print(embedding.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0192,  0.0456,  0.0017,  ..., -0.0313,  0.0531, -0.0389],\n",
       "        [-0.0191,  0.0401, -0.0203,  ..., -0.0047,  0.1012, -0.0017],\n",
       "        [-0.0140,  0.0728, -0.0112,  ..., -0.0900,  0.0288,  0.0364],\n",
       "        ...,\n",
       "        [ 0.0071, -0.0141, -0.0514,  ..., -0.0081, -0.0669, -0.0096],\n",
       "        [ 0.0232, -0.0285, -0.0173,  ..., -0.0430,  0.0374, -0.0250],\n",
       "        [-0.0344, -0.0044, -0.0261,  ..., -0.1030,  0.0151,  0.0507]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = embedding.weight.shape[0]\n",
    "permutation = torch.randperm(vocab_size)\n",
    "\n",
    "embedding.weight.data = embedding.weight.data[permutation, :]\n",
    "model.base_model.set_input_embeddings(embedding)\n",
    "model.base_model.get_input_embeddings().weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLMRobertaTokenizerFast' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39m../models/afriberta_rand\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m tokenizer_afriberta\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39m\u001b[39m../models/afriberta_rand/tokenizer.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMRobertaTokenizerFast' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"../models/afriberta_rand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current max_split_size_mb: N/A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_value = os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\", \"N/A\")\n",
    "print(\"Current max_split_size_mb:\", current_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0192,  0.0456,  0.0017,  ..., -0.0313,  0.0531, -0.0389],\n",
       "        [-0.0191,  0.0401, -0.0203,  ..., -0.0047,  0.1012, -0.0017],\n",
       "        [-0.0140,  0.0728, -0.0112,  ..., -0.0900,  0.0288,  0.0364],\n",
       "        ...,\n",
       "        [ 0.0071, -0.0141, -0.0514,  ..., -0.0081, -0.0669, -0.0096],\n",
       "        [ 0.0232, -0.0285, -0.0173,  ..., -0.0430,  0.0374, -0.0250],\n",
       "        [-0.0344, -0.0044, -0.0261,  ..., -0.1030,  0.0151,  0.0507]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afriberta_rand = AutoModelForMaskedLM.from_pretrained(\"../models/afriberta_rand\")\n",
    "afriberta_rand.base_model.get_input_embeddings().weight.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
